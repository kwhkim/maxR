<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>missing on R to the max</title>
    <link>https://kwhkim.github.io/maxR/tags/missing/</link>
    <description>Recent content in missing on R to the max</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 25 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://kwhkim.github.io/maxR/tags/missing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why mean substition is a bad idea, almost always</title>
      <link>https://kwhkim.github.io/maxR/2022/03/25/why-single-mean-imputation-is-a-bad-idea-almost-always/</link>
      <pubDate>Fri, 25 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://kwhkim.github.io/maxR/2022/03/25/why-single-mean-imputation-is-a-bad-idea-almost-always/</guid>
      <description>
&lt;script src=&#34;https://kwhkim.github.io/maxR/2022/03/25/why-single-mean-imputation-is-a-bad-idea-almost-always/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Missing values can cause bias. So most books introduce imputation methods like mean substitution or LOCF(&lt;strong&gt;L&lt;/strong&gt;ast &lt;strong&gt;O&lt;/strong&gt;bservation &lt;strong&gt;C&lt;/strong&gt;arried &lt;strong&gt;F&lt;/strong&gt;orward). But in this post, I will explain why people say unconditional mean substitution is bad with a simple example.&lt;/p&gt;
&lt;div id=&#34;mechanisms-of-missingness&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mechanisms of Missingness&lt;/h2&gt;
&lt;p&gt;Little and Rubin(2002) categorized missingness into three categories.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;MCAR(&lt;strong&gt;M&lt;/strong&gt;issing &lt;strong&gt;C&lt;/strong&gt;ompmletely &lt;strong&gt;A&lt;/strong&gt;t &lt;strong&gt;R&lt;/strong&gt;andom)&lt;/li&gt;
&lt;li&gt;MAR(&lt;strong&gt;M&lt;/strong&gt;issing &lt;strong&gt;A&lt;/strong&gt;t &lt;strong&gt;R&lt;/strong&gt;andom)&lt;/li&gt;
&lt;li&gt;NMAR(&lt;strong&gt;N&lt;/strong&gt;ot &lt;strong&gt;M&lt;/strong&gt;issing &lt;strong&gt;A&lt;/strong&gt;t &lt;strong&gt;R&lt;/strong&gt;andom)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In simple terms, MCAR is missing &lt;strong&gt;unconditionally&lt;/strong&gt; random, MAR is missing &lt;strong&gt;conditionally&lt;/strong&gt; random. And NMAR is neither of the both.&lt;/p&gt;
&lt;p&gt;Here is a (unrealistic, but) simple example model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\textrm{Weight} = 0.48 \times \textrm{Height} + e\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I will cover only the missing weight values in this post. Assume that we can somehow find out what the real value is even if it is missing.&lt;/p&gt;
&lt;p&gt;If missingness of weight is &lt;strong&gt;not related to other variables including itself&lt;/strong&gt;, it is called &lt;strong&gt;MCAR&lt;/strong&gt;. It is like missingness is &lt;strong&gt;totally determined by flipping coins&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If missingness of weight is &lt;strong&gt;conditional on height(and other variables in the model) but independent of the weight value&lt;/strong&gt;, it is called &lt;strong&gt;MAR&lt;/strong&gt;. Overall distribution of weight can be different dependent on missingness, but given the information of height(and other variables in the model), it is identical. Missing is independent of weight, given the height(and other variables in the model). Given the height (and other variables in the model), missing occurs independent of the height. So we can say missingness is &lt;strong&gt;determined by flipping coins conditional on the value of the height(and other variables in the model)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If we digest the above using some of the basic probability rules, we get to the conclusion below.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Missing value distribution is not different from observed value distribution, given the value of other variables in the model.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The following might be true.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(y|y_{\textrm{missing}}) \neq p(y|y_{\textrm{observed}})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But the following holds true.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(y|y_{\textrm{missing}}, x_1, x_2, \cdots, x_p) = p(y|y_{\textrm{observed}}, x_1, x_2, \cdots, x_p)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is like flipping coin again. But it could be a different coin for different value of explanatory variables.&lt;/p&gt;
&lt;p&gt;If it is neither MCAR nor MAR, it is called &lt;strong&gt;NMAR&lt;/strong&gt;. For example, if the missingness of weight is dependent on weight itself, it is NMAR.&lt;/p&gt;
&lt;p&gt;For predictive models, it is sufficient of check if missingness is random conditional on other observed variables. But for causal model, we should consider if there is unobserved variables that might cause or be related to missingness. For now, I will consider only predictive models.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;strong&gt;AD&lt;/strong&gt;] Book for &lt;strong&gt;R power users&lt;/strong&gt; : &lt;a href=&#34;http://books.sumeun.org/?p=190&#34;&gt;Data Analysis with R: Data Preprocessing and Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;why-not-just-use-complete-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why not just use complete data?&lt;/h2&gt;
&lt;p&gt;The main problem is &lt;strong&gt;the bias&lt;/strong&gt; introduced by missing data. As the number of missing increases, the bias could be huge. Another problem is decreasing sample size. Smaller sample size means &lt;strong&gt;less power&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-was-missing-data-handled-traditionally&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How was missing data handled, traditionally?&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Listwise Deletion : Use only complete data&lt;/li&gt;
&lt;li&gt;Pairwise Deletion : Use all data available for each analysis&lt;/li&gt;
&lt;li&gt;Unconditional mean substitution&lt;/li&gt;
&lt;li&gt;Regression Imputation(Conditional mean substitution)&lt;/li&gt;
&lt;li&gt;Stochastic Regression Imputation&lt;/li&gt;
&lt;li&gt;Hot-Deck Imputation&lt;/li&gt;
&lt;li&gt;Last Observation Carried Forward&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Listwise Deletion&lt;/strong&gt; means using only complete data. Ignore any data with at least one missing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pairwise Deletion&lt;/strong&gt; means using all available data for each estimates. Let’s say we need to compute covariance matrix of variables &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; , &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; , and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; . We need to compute covariance of each pairs. We can use data missing in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for computing covariance of &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; . This method can cause the problem of non-positive definite covariance matrix estimate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unconditional mean substitution&lt;/strong&gt; imputes missing with the variable mean.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regression Imputation&lt;/strong&gt; utilizes regression analysis and impute missing with regression mean.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stochastic Regression Imputation&lt;/strong&gt; also uses regression analysis. It imputes missing with additional stochastic error term.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hot-Deck Imputation&lt;/strong&gt; imputes missing with values from other complete data. Wikipedia describes it as below.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A once-common method of imputation was hot-deck imputation where a missing value was imputed from a &lt;strong&gt;randomly selected similar record&lt;/strong&gt;. The term “hot deck” dates back to the storage of data on punched cards, and indicates that the information donors come from the same dataset as the recipients.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Statisticians at the Census Bureau originally developed the hot-deck to deal with missing data in public-use data sets, and the procedure has a long history in survey applications (Scheuren, 2005; Enders, 2010).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Last Observation Carried Forward&lt;/strong&gt; imputes missing with last observed value for the same group in the logitudinal data.&lt;/p&gt;
&lt;p&gt;Which one of those missing data analysis above is unbiased depends on the &lt;strong&gt;whether missing is MCAR, MAR, or NMAR&lt;/strong&gt; and &lt;strong&gt;what one is estimating using the analysis&lt;/strong&gt;. For instance, for estimating a regression parameters &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(y=b_0 + b_1 x_1\)&lt;/span&gt;, listwise deletion is fairly appropriate for MCAR or MAR data, unless we care much about losing power. But estimating the mean &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; averaging only observed &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; could be seriously biased for MAR data if we use listwise deletion.&lt;/p&gt;
&lt;p&gt;Deletion or imputation methods above result in complete data and we can use complete data analysis method. That’s why we prefer imputation or deletion more than other special methods developed for dealing with missing data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-mean-y-mar-listwise-deletion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating mean &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; : MAR &amp;amp; listwise deletion&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is missing complete random, estimating &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; using only observed data is okay because &lt;span class=&#34;math inline&#34;&gt;\(p(y|y_\textrm{missing}) = p(y|y_\textrm{observed})\)&lt;/span&gt; .&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-mean-y-mar-mean-substitution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating mean &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; : MAR &amp;amp; mean substitution&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is missing conditionally random, using only observed data could be problematic because &lt;span class=&#34;math inline&#34;&gt;\(p(y|y_\textrm{missing}) \neq p(y|y_\textrm{observed})\)&lt;/span&gt; . Let’s say &lt;span class=&#34;math inline&#34;&gt;\(p(\textrm{missing}) \sim \textrm{height}\)&lt;/span&gt; . The probability of missing on weight increases as the height increases. In that case, the probability of missing weight is increased as the weight increases. So just using the complete data means deleting higher weight values and introducing a bias on estimating the mean weight.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-mean-y-mar-mean-substitution-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating mean &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; : MAR &amp;amp; mean substitution&lt;/h2&gt;
&lt;p&gt;So we would better be using regression mean(estimated weight mean given the height). Using the conditional mean for missing data might lead to too small variance estimate but it is not biased.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;strong&gt;AD&lt;/strong&gt;] Book for &lt;strong&gt;R power users&lt;/strong&gt; : &lt;a href=&#34;http://books.sumeun.org/?p=190&#34;&gt;Data Analysis with R: Data Preprocessing and Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 4.1.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 4.1.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample size 100
n &amp;lt;- 100
# height mean 170, std 15
h &amp;lt;- rnorm(100, 170, 15)  
# true relation : weight =  0.48 * height
# given height, weight distribution N(0,7^2)
w &amp;lt;- 0.48 * h  + rnorm(n, 0, 7)

# weight population mean
w_pop &amp;lt;- 170*0.48 # 81.6
h_pop &amp;lt;- 170
# missing is dependent on height
w_missing &amp;lt;- runif(n, 0, 1) &amp;lt; (h-min(h))/(max(h)-min(h)) 

dat = data.frame(h=h,
                 w=ifelse(w_missing, NA, w),
                 w_complete = w,
                 w_missing = w_missing)

#dat %&amp;gt;% gather()
ggplot(dat, aes(x=h, y=w_complete, col=factor(w_missing))) + 
  geom_point() + 
  scale_color_manual(values=c(&amp;#39;black&amp;#39;, &amp;#39;grey&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kwhkim.github.io/maxR/2022/03/25/why-single-mean-imputation-is-a-bad-idea-almost-always/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;listwise-deletion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Listwise deletion&lt;/h3&gt;
&lt;p&gt;Estimated weight mean from using complete data seems biased.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## average weight?
mean(dat$w_complete)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 80.40551&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wmean_est = mean(dat$w, na.rm = TRUE)
wmean_est&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 77.90547&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(dat$w)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  dat$w
## t = 51.013, df = 47, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  74.83319 80.97774
## sample estimates:
## mean of x 
##  77.90547&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mean-substitution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mean substitution&lt;/h3&gt;
&lt;p&gt;Simple but bad alternative in terms of bias for MAR data is mean susbstitution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;w_mean &amp;lt;- mean(dat$w, na.rm=TRUE)
dat$w_imputed &amp;lt;- ifelse(dat$w_missing, w_mean, w)

wmean_est = mean(dat$w_imputed)
wmean_est&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 77.90547&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- t.test(dat$w_imputed)
res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  dat$w_imputed
## t = 106.86, df = 99, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  76.45893 79.35200
## sample estimates:
## mean of x 
##  77.90547&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The population weight mean is 81.6 but the estimated mean is 77.91. The confidence inteval is 76.46-79.35.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-imputation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression imputation&lt;/h3&gt;
&lt;p&gt;We can use regression model to imput the missing values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- lm(w ~ h)
w_hat &amp;lt;- predict(mod, dat)
#w_hat &amp;lt;- coef(mod) %*% rbind(1,dat$h)
dat$w_imputed &amp;lt;- ifelse(dat$w_missing, w_hat, w)

wmean_est = mean(dat$w_imputed)
wmean_est&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 80.44631&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- t.test(dat$w_imputed)
res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  dat$w_imputed
## t = 84.369, df = 99, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  78.55436 82.33827
## sample estimates:
## mean of x 
##  80.44631&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Estimated weight value is 80.45, 95%-confidence interval is 78.55-82.34. Compare this with estimated weight mean from listwise-deletion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(dat$w_complete)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 80.40551&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(dat$w_complete)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  dat$w_complete
## t = 75.707, df = 99, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  78.29816 82.51286
## sample estimates:
## mean of x 
##  80.40551&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Here are some explanatory posts and a paper that show how the causal model can be beneficial to understanding missing mechanisms: &lt;a href=&#34;https://www.rdatagen.net/post/musings-on-missing-data/&#34;&gt;Musings on missing data&lt;/a&gt;, &lt;a href=&#34;http://jakewestfall.org/blog/index.php/2017/08/22/using-causal-graphs-to-understand-missingness-and-how-to-deal-with-it/&#34;&gt;Using causal graphs to understand missingness and how to deal with it&lt;/a&gt;, &lt;a href=&#34;https://proceedings.neurips.cc/paper/2013/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf&#34;&gt;Graphical Models for Inference with Missing Data&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;In fact, simulation studies suggest that mean imputation is possibly the worst missing data handling method available(Enders, 2010).&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <tag>missing</tag>
      
      
            <category>R</category>
      
    </item>
    
  </channel>
</rss>