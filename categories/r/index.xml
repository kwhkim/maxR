<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on R to the max</title>
    <link>https://kwhkim.github.io/maxR/categories/r/</link>
    <description>Recent content in R on R to the max</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 25 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://kwhkim.github.io/maxR/categories/r/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why mean substition is a bad idea, almost always</title>
      <link>https://kwhkim.github.io/maxR/2022/03/25/why-single-mean-imputation-is-a-bad-idea-almost-always/</link>
      <pubDate>Fri, 25 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://kwhkim.github.io/maxR/2022/03/25/why-single-mean-imputation-is-a-bad-idea-almost-always/</guid>
      <description>
&lt;script src=&#34;https://kwhkim.github.io/maxR/2022/03/25/why-single-mean-imputation-is-a-bad-idea-almost-always/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Missing values can cause bias. So most books introduce imputation methods like mean substitution or LOCF(&lt;strong&gt;L&lt;/strong&gt;ast &lt;strong&gt;O&lt;/strong&gt;bservation &lt;strong&gt;C&lt;/strong&gt;arried &lt;strong&gt;F&lt;/strong&gt;orward). But in this post, I will explain why people say unconditional mean substitution is bad with a simple example.&lt;/p&gt;
&lt;div id=&#34;mechanisms-of-missingness&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mechanisms of Missingness&lt;/h2&gt;
&lt;p&gt;Little and Rubin(2002) categorized missingness into three categories.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;MCAR(&lt;strong&gt;M&lt;/strong&gt;issing &lt;strong&gt;C&lt;/strong&gt;ompmletely &lt;strong&gt;A&lt;/strong&gt;t &lt;strong&gt;R&lt;/strong&gt;andom)&lt;/li&gt;
&lt;li&gt;MAR(&lt;strong&gt;M&lt;/strong&gt;issing &lt;strong&gt;A&lt;/strong&gt;t &lt;strong&gt;R&lt;/strong&gt;andom)&lt;/li&gt;
&lt;li&gt;NMAR(&lt;strong&gt;N&lt;/strong&gt;ot &lt;strong&gt;M&lt;/strong&gt;issing &lt;strong&gt;A&lt;/strong&gt;t &lt;strong&gt;R&lt;/strong&gt;andom)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In simple terms, MCAR is missing &lt;strong&gt;unconditionally&lt;/strong&gt; random, MAR is missing &lt;strong&gt;conditionally&lt;/strong&gt; random. And NMAR is neither of the both.&lt;/p&gt;
&lt;p&gt;Here is a (unrealistic, but) simple example model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\textrm{Weight} = 0.48 \times \textrm{Height} + e\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I will cover only the missing weight values in this post. Assume that we can somehow find out what the real value is even if it is missing.&lt;/p&gt;
&lt;p&gt;If missingness of weight is &lt;strong&gt;not related to other variables including itself&lt;/strong&gt;, it is called &lt;strong&gt;MCAR&lt;/strong&gt;. It is like missingness is &lt;strong&gt;totally determined by flipping coins&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If missingness of weight is &lt;strong&gt;conditional on height(and other variables in the model) but independent of the weight value&lt;/strong&gt;, it is called &lt;strong&gt;MAR&lt;/strong&gt;. Overall distribution of weight can be different dependent on missingness, but given the information of height(and other variables in the model), it is identical. Missing is independent of weight, given the height(and other variables in the model). Given the height (and other variables in the model), missing occurs independent of the height. So we can say missingness is &lt;strong&gt;determined by flipping coins conditional on the value of the height(and other variables in the model)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If we digest the above using some of the basic probability rules, we get to the conclusion below.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Missing value distribution is not different from observed value distribution, given the value of other variables in the model.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The following might be true.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(y|y_{\textrm{missing}}) \neq p(y|y_{\textrm{observed}})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But the following holds true.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(y|y_{\textrm{missing}}, x_1, x_2, \cdots, x_p) = p(y|y_{\textrm{observed}}, x_1, x_2, \cdots, x_p)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is like flipping coin again. But it could be a different coin for different value of explanatory variables.&lt;/p&gt;
&lt;p&gt;If it is neither MCAR nor MAR, it is called &lt;strong&gt;NMAR&lt;/strong&gt;. For example, if the missingness of weight is dependent on weight itself, it is NMAR.&lt;/p&gt;
&lt;p&gt;For predictive models, it is sufficient of check if missingness is random conditional on other observed variables. But for causal model, we should consider if there is unobserved variables that might cause or be related to missingness. For now, I will consider only predictive models.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;strong&gt;AD&lt;/strong&gt;] Book for &lt;strong&gt;R power users&lt;/strong&gt; : &lt;a href=&#34;http://books.sumeun.org/?p=190&#34;&gt;Data Analysis with R: Data Preprocessing and Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;why-not-just-use-complete-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why not just use complete data?&lt;/h2&gt;
&lt;p&gt;The main problem is &lt;strong&gt;the bias&lt;/strong&gt; introduced by missing data. As the number of missing increases, the bias could be huge. Another problem is decreasing sample size. Smaller sample size means &lt;strong&gt;less power&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-was-missing-data-handled-traditionally&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How was missing data handled, traditionally?&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Listwise Deletion : Use only complete data&lt;/li&gt;
&lt;li&gt;Pairwise Deletion : Use all data available for each analysis&lt;/li&gt;
&lt;li&gt;Unconditional mean substitution&lt;/li&gt;
&lt;li&gt;Regression Imputation(Conditional mean substitution)&lt;/li&gt;
&lt;li&gt;Stochastic Regression Imputation&lt;/li&gt;
&lt;li&gt;Hot-Deck Imputation&lt;/li&gt;
&lt;li&gt;Last Observation Carried Forward&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Listwise Deletion&lt;/strong&gt; means using only complete data. Ignore any data with at least one missing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pairwise Deletion&lt;/strong&gt; means using all available data for each estimates. Let’s say we need to compute covariance matrix of variables &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; , &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; , and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; . We need to compute covariance of each pairs. We can use data missing in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for computing covariance of &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; . This method can cause the problem of non-positive definite covariance matrix estimate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unconditional mean substitution&lt;/strong&gt; imputes missing with the variable mean.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regression Imputation&lt;/strong&gt; utilizes regression analysis and impute missing with regression mean.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stochastic Regression Imputation&lt;/strong&gt; also uses regression analysis. It imputes missing with additional stochastic error term.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hot-Deck Imputation&lt;/strong&gt; imputes missing with values from other complete data. Wikipedia describes it as below.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A once-common method of imputation was hot-deck imputation where a missing value was imputed from a &lt;strong&gt;randomly selected similar record&lt;/strong&gt;. The term “hot deck” dates back to the storage of data on punched cards, and indicates that the information donors come from the same dataset as the recipients.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Statisticians at the Census Bureau originally developed the hot-deck to deal with missing data in public-use data sets, and the procedure has a long history in survey applications (Scheuren, 2005; Enders, 2010).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Last Observation Carried Forward&lt;/strong&gt; imputes missing with last observed value for the same group in the logitudinal data.&lt;/p&gt;
&lt;p&gt;Which one of those missing data analysis above is unbiased depends on the &lt;strong&gt;whether missing is MCAR, MAR, or NMAR&lt;/strong&gt; and &lt;strong&gt;what one is estimating using the analysis&lt;/strong&gt;. For instance, for estimating a regression parameters &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(y=b_0 + b_1 x_1\)&lt;/span&gt;, listwise deletion is fairly appropriate for MCAR or MAR data, unless we care much about losing power. But estimating the mean &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; averaging only observed &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; could be seriously biased for MAR data if we use listwise deletion.&lt;/p&gt;
&lt;p&gt;Deletion or imputation methods above result in complete data and we can use complete data analysis method. That’s why we prefer imputation or deletion more than other special methods developed for dealing with missing data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-mean-y-mar-listwise-deletion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating mean &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; : MAR &amp;amp; listwise deletion&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is missing complete random, estimating &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; using only observed data is okay because &lt;span class=&#34;math inline&#34;&gt;\(p(y|y_\textrm{missing}) = p(y|y_\textrm{observed})\)&lt;/span&gt; .&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-mean-y-mar-mean-substitution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating mean &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; : MAR &amp;amp; mean substitution&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is missing conditionally random, using only observed data could be problematic because &lt;span class=&#34;math inline&#34;&gt;\(p(y|y_\textrm{missing}) \neq p(y|y_\textrm{observed})\)&lt;/span&gt; . Let’s say &lt;span class=&#34;math inline&#34;&gt;\(p(\textrm{missing}) \sim \textrm{height}\)&lt;/span&gt; . The probability of missing on weight increases as the height increases. In that case, the probability of missing weight is increased as the weight increases. So just using the complete data means deleting higher weight values and introducing a bias on estimating the mean weight.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-mean-y-mar-mean-substitution-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating mean &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; : MAR &amp;amp; mean substitution&lt;/h2&gt;
&lt;p&gt;So we would better be using regression mean(estimated weight mean given the height). Using the conditional mean for missing data might lead to too small variance estimate but it is not biased.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;strong&gt;AD&lt;/strong&gt;] Book for &lt;strong&gt;R power users&lt;/strong&gt; : &lt;a href=&#34;http://books.sumeun.org/?p=190&#34;&gt;Data Analysis with R: Data Preprocessing and Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 4.1.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 4.1.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample size 100
n &amp;lt;- 100
# height mean 170, std 15
h &amp;lt;- rnorm(100, 170, 15)  
# true relation : weight =  0.48 * height
# given height, weight distribution N(0,7^2)
w &amp;lt;- 0.48 * h  + rnorm(n, 0, 7)

# weight population mean
w_pop &amp;lt;- 170*0.48 # 81.6
h_pop &amp;lt;- 170
# missing is dependent on height
w_missing &amp;lt;- runif(n, 0, 1) &amp;lt; (h-min(h))/(max(h)-min(h)) 

dat = data.frame(h=h,
                 w=ifelse(w_missing, NA, w),
                 w_complete = w,
                 w_missing = w_missing)

#dat %&amp;gt;% gather()
ggplot(dat, aes(x=h, y=w_complete, col=factor(w_missing))) + 
  geom_point() + 
  scale_color_manual(values=c(&amp;#39;black&amp;#39;, &amp;#39;grey&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kwhkim.github.io/maxR/2022/03/25/why-single-mean-imputation-is-a-bad-idea-almost-always/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;listwise-deletion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Listwise deletion&lt;/h3&gt;
&lt;p&gt;Estimated weight mean from using complete data seems biased.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## average weight?
mean(dat$w_complete)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 80.40551&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wmean_est = mean(dat$w, na.rm = TRUE)
wmean_est&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 77.90547&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(dat$w)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  dat$w
## t = 51.013, df = 47, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  74.83319 80.97774
## sample estimates:
## mean of x 
##  77.90547&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mean-substitution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mean substitution&lt;/h3&gt;
&lt;p&gt;Simple but bad alternative in terms of bias for MAR data is mean susbstitution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;w_mean &amp;lt;- mean(dat$w, na.rm=TRUE)
dat$w_imputed &amp;lt;- ifelse(dat$w_missing, w_mean, w)

wmean_est = mean(dat$w_imputed)
wmean_est&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 77.90547&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- t.test(dat$w_imputed)
res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  dat$w_imputed
## t = 106.86, df = 99, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  76.45893 79.35200
## sample estimates:
## mean of x 
##  77.90547&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The population weight mean is 81.6 but the estimated mean is 77.91. The confidence inteval is 76.46-79.35.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-imputation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression imputation&lt;/h3&gt;
&lt;p&gt;We can use regression model to imput the missing values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- lm(w ~ h)
w_hat &amp;lt;- predict(mod, dat)
#w_hat &amp;lt;- coef(mod) %*% rbind(1,dat$h)
dat$w_imputed &amp;lt;- ifelse(dat$w_missing, w_hat, w)

wmean_est = mean(dat$w_imputed)
wmean_est&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 80.44631&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- t.test(dat$w_imputed)
res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  dat$w_imputed
## t = 84.369, df = 99, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  78.55436 82.33827
## sample estimates:
## mean of x 
##  80.44631&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Estimated weight value is 80.45, 95%-confidence interval is 78.55-82.34. Compare this with estimated weight mean from listwise-deletion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(dat$w_complete)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 80.40551&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(dat$w_complete)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  dat$w_complete
## t = 75.707, df = 99, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  78.29816 82.51286
## sample estimates:
## mean of x 
##  80.40551&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Here are some explanatory posts and a paper that show how the causal model can be beneficial to understanding missing mechanisms: &lt;a href=&#34;https://www.rdatagen.net/post/musings-on-missing-data/&#34;&gt;Musings on missing data&lt;/a&gt;, &lt;a href=&#34;http://jakewestfall.org/blog/index.php/2017/08/22/using-causal-graphs-to-understand-missingness-and-how-to-deal-with-it/&#34;&gt;Using causal graphs to understand missingness and how to deal with it&lt;/a&gt;, &lt;a href=&#34;https://proceedings.neurips.cc/paper/2013/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf&#34;&gt;Graphical Models for Inference with Missing Data&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;In fact, simulation studies suggest that mean imputation is possibly the worst missing data handling method available(Enders, 2010).&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <tag>missing</tag>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>measurement units</title>
      <link>https://kwhkim.github.io/maxR/2022/03/15/measurement-units/</link>
      <pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://kwhkim.github.io/maxR/2022/03/15/measurement-units/</guid>
      <description>
&lt;script src=&#34;https://kwhkim.github.io/maxR/2022/03/15/measurement-units/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Data &lt;code&gt;mtcars&lt;/code&gt; has a column named &lt;code&gt;mpg&lt;/code&gt;. &lt;code&gt;mpg&lt;/code&gt; means &lt;strong&gt;m&lt;/strong&gt;iles &lt;strong&gt;p&lt;/strong&gt;er &lt;strong&gt;g&lt;/strong&gt;allon. ‘Mile’ and ‘gallon’ are units for length and volume. A mile is approximately 1.6 kilometers and a gallon is approximately 3.7 liters. Mile and gallon sound unfamiliar to people who live outside England or U.S.A. because international standard units for length and volume are meter and liter.&lt;/p&gt;
&lt;p&gt;In this post, we will learn how to convert a unit to another unit, for instance, we will convert mpg to km/L, which is more comprehensible to people who use SI units.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;units-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Units in R&lt;/h2&gt;
&lt;p&gt;Vectors(the most common data structure in R) do not contain information of measurement units. Units are implicit and units should be converted by users. But as history tells us, unit conversion should be treated carefully because it can cause serious damage to the whole project&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;package-units&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Package &lt;code&gt;units&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Using the package &lt;code&gt;units&lt;/code&gt;, we can easily convert units accurately. And the data is plotted, units will be included in the x- or y-label automatically.&lt;/p&gt;
&lt;p&gt;First install package &lt;code&gt;units&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;#39;units&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And load all the necessary packages and data&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(units)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## udunits database from C:/Users/Seul/Documents/R/win-library/4.1/units/share/udunits/udunits2.xml&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr, warn.conflicts = FALSE)
data(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get the information about the data &lt;code&gt;mtcars&lt;/code&gt;, we can do &lt;code&gt;help(mtcars)&lt;/code&gt;. It will show the measurement unit for each column. &lt;code&gt;mpg&lt;/code&gt; is measured in unit of &lt;strong&gt;m&lt;/strong&gt;iles &lt;strong&gt;p&lt;/strong&gt;er &lt;strong&gt;g&lt;/strong&gt;allon, &lt;code&gt;disp&lt;/code&gt; is measured in unit of cubic inch, &lt;code&gt;hp&lt;/code&gt; is measured in unit of gross &lt;strong&gt;h&lt;/strong&gt;orse&lt;strong&gt;p&lt;/strong&gt;ower, &lt;code&gt;wt&lt;/code&gt; is measured in unit of 1000 lbs, and &lt;code&gt;qsec&lt;/code&gt; is measured in unit of sec per 1/4 mile.&lt;/p&gt;
&lt;p&gt;It is sad that mpg(&lt;strong&gt;m&lt;/strong&gt;iles &lt;strong&gt;p&lt;/strong&gt;er &lt;strong&gt;g&lt;/strong&gt;allon) is not registered in the package &lt;code&gt;units&lt;/code&gt;, but we can register it ourselves. The code below installs a new unit called &lt;code&gt;mpg_US&lt;/code&gt; as &lt;code&gt;interantional_mile/US_liquid_gallon&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install_unit(name=&amp;#39;mpg_US&amp;#39;, def=&amp;#39;international_mile/US_liquid_gallon&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can use &lt;code&gt;mpg_US&lt;/code&gt;. &lt;code&gt;mtcars$mpg&lt;/code&gt; is measured in unit of mpg(US) and &lt;code&gt;mtcars$wt&lt;/code&gt; is measured in unit of kilogram.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;units(mtcars$mpg) = &amp;#39;mpg_US&amp;#39;
units(mtcars$wt) = &amp;#39;kg&amp;#39;
mtcars$mpg %&amp;gt;% head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Units: [mpg_US]
## [1] 21.0 21.0 22.8 21.4 18.7 18.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to convert the unit mpg(US) to SI unit km/L, we need to do the below.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;units(mtcars$mpg) = &amp;#39;km/L&amp;#39;
mtcars$mpg %&amp;gt;% head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Units: [km/L]
## [1] 8.928017 8.928017 9.693276 9.098075 7.950187 7.695101&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can easily plot the relation between &lt;code&gt;mpg&lt;/code&gt; and &lt;code&gt;wt&lt;/code&gt; using the package &lt;code&gt;ggplot2&lt;/code&gt;. But do not forget to load &lt;code&gt;ggforce&lt;/code&gt; beforehand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggforce) # without this, the code below will raise error!&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;ggforce&amp;#39;:
##   method           from 
##   scale_type.units units&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data=mtcars,
       aes(x=mpg, y=wt)) + 
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kwhkim.github.io/maxR/2022/03/15/measurement-units/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;units::units()&amp;lt;-&lt;/code&gt; for setting unit for measurements.
&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;units::units()&amp;lt;-&lt;/code&gt; for converting unit.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;units::units()&amp;lt;-NULL&lt;/code&gt; for deleting unit.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;install_unit(name=, def=)&lt;/code&gt; for introducing new units.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;valid_udunits()&lt;/code&gt; to show all the units available from the package &lt;code&gt;units&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;valid_udunits() %&amp;gt;% head &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## udunits database from C:/Users/Seul/Documents/R/win-library/4.1/units/share/udunits/udunits2.xml&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 11
##   symbol symbol_aliases name_singular name_singular_aliases name_plural
##   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;      
## 1 m      &amp;quot;&amp;quot;             meter         &amp;quot;metre&amp;quot;               &amp;quot;&amp;quot;         
## 2 kg     &amp;quot;&amp;quot;             kilogram      &amp;quot;&amp;quot;                    &amp;quot;&amp;quot;         
## 3 s      &amp;quot;&amp;quot;             second        &amp;quot;&amp;quot;                    &amp;quot;&amp;quot;         
## 4 A      &amp;quot;&amp;quot;             ampere        &amp;quot;&amp;quot;                    &amp;quot;&amp;quot;         
## 5 K      &amp;quot;&amp;quot;             kelvin        &amp;quot;&amp;quot;                    &amp;quot;&amp;quot;         
## 6 mol    &amp;quot;&amp;quot;             mole          &amp;quot;&amp;quot;                    &amp;quot;&amp;quot;         
## # ... with 6 more variables: name_plural_aliases &amp;lt;chr&amp;gt;, def &amp;lt;chr&amp;gt;,
## #   definition &amp;lt;chr&amp;gt;, comment &amp;lt;chr&amp;gt;, dimensionless &amp;lt;lgl&amp;gt;, source_xml &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/International_System_of_Units&#34;&gt;International System of Units&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;It is well known that the failure of MCO(&lt;strong&gt;M&lt;/strong&gt;ars &lt;strong&gt;C&lt;/strong&gt;limate &lt;strong&gt;O&lt;/strong&gt;rbiter) is due to inadequate unit coversion.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;If the objective is simply to reset the unit, do &lt;code&gt;units(mtcars$mpg)=NULL; units(mtcars$mpg)=&#39;km/L&#39;&lt;/code&gt;. This will not convert unit but just replace the unit with another unit.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <tag>visualization</tag>
      
            <tag>preprocessing</tag>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Better Visualization of y|x for Big Data</title>
      <link>https://kwhkim.github.io/maxR/2022/03/06/better-visualization-of-y-x-for-big-data/</link>
      <pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://kwhkim.github.io/maxR/2022/03/06/better-visualization-of-y-x-for-big-data/</guid>
      <description>
&lt;script src=&#34;https://kwhkim.github.io/maxR/2022/03/06/better-visualization-of-y-x-for-big-data/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;plotting-big-data-and-alpha&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting Big Data and Alpha&lt;/h2&gt;
&lt;p&gt;When plotting too many data points, we use &lt;code&gt;alpha=&lt;/code&gt; because points are overlapped and indistinguishable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(data.table)
library(cowplot)
library(ggplot2)

#N &amp;lt;- 1000
N &amp;lt;- 1000000

x &amp;lt;- rnorm(N)
y &amp;lt;- x + rnorm(N)


dat &amp;lt;- data.table(x = x, 
                  y = y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% ggplot(aes(x=x, y=y)) + 
  geom_point() + 
  labs(title=&amp;#39;Original plot&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kwhkim.github.io/maxR/2022/03/06/better-visualization-of-y-x-for-big-data/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% ggplot(aes(x=x, y=y)) + geom_point(alpha=0.01) + 
  labs(title=&amp;#39;Using alpha=0.01&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kwhkim.github.io/maxR/2022/03/06/better-visualization-of-y-x-for-big-data/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Meaningful minimal &lt;code&gt;alpha&lt;/code&gt; seems to be &lt;code&gt;0.01&lt;/code&gt; for &lt;code&gt;ggplot2&lt;/code&gt;. For very big data, &lt;code&gt;alpha=0.01&lt;/code&gt; is not small enough. Looking at the plot the above, we see big blackness in the center. This might be that the densities in the center are the same or it might be that they reached to the ceiling of blackness even if the densities are not equal.&lt;/p&gt;
&lt;p&gt;Bivariate normal distribution is too simple. Let’s try more complex data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x1 &amp;lt;- rnorm(N/2)
y1 &amp;lt;- 2*sin(x1) + rnorm(N/2)
x2 &amp;lt;- rnorm(N/2)
y2 &amp;lt;- 2*cos(x2) + rt(N/2, df=30)

dat &amp;lt;- data.table(x=c(x1,x2),
                  y=c(y1,y2))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;using-multiple-alphas&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using multiple &lt;code&gt;alpha&lt;/code&gt;s&lt;/h3&gt;
&lt;p&gt;We can use multiple &lt;code&gt;alpha&lt;/code&gt;s to avoid the problem of ceiling effect of constant &lt;code&gt;alpha&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- dat %&amp;gt;% ggplot(aes(x=x, y=y)) + geom_point() + 
  labs(title=&amp;#39;alpha=1&amp;#39;) + theme_minimal()
p2 &amp;lt;- dat %&amp;gt;% ggplot(aes(x=x, y=y)) + geom_point(alpha=0.1) + 
  labs(title=&amp;#39;alpha=0.1&amp;#39;) + theme_minimal()
p3 &amp;lt;- dat %&amp;gt;% ggplot(aes(x=x, y=y)) + geom_point(alpha=0.05) +
  labs(title=&amp;#39;alpha=0.05&amp;#39;) + theme_minimal()
p4 &amp;lt;- dat %&amp;gt;% ggplot(aes(x=x, y=y)) + geom_point(alpha=0.01) +
  labs(title=&amp;#39;alpha=0.01&amp;#39;) + theme_minimal()

plot_grid(p1,p2,p3,p4,ncol=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kwhkim.github.io/maxR/2022/03/06/better-visualization-of-y-x-for-big-data/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But using the minimal &lt;code&gt;alpha=0.01&lt;/code&gt; does not reveal the density differences in the center. We can try sampling in this case.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sampling&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- dat %&amp;gt;% sample_n(N/5) %&amp;gt;% 
  ggplot(aes(x=x, y=y)) + geom_point(alpha=0.01) + 
  labs(title=&amp;#39;alpha=0.01 with 20% of data&amp;#39;) + theme_minimal()
p2 &amp;lt;- dat %&amp;gt;% sample_n(N/10) %&amp;gt;% 
  ggplot(aes(x=x, y=y)) + geom_point(alpha=0.01) + 
  labs(title=&amp;#39;alpha=0.01 with 10% of data&amp;#39;) + theme_minimal()
p3 &amp;lt;- dat %&amp;gt;% sample_n(N/50) %&amp;gt;% 
  ggplot(aes(x=x, y=y)) + geom_point(alpha=0.01) +
  labs(title=&amp;#39;alpha=0.01 with 2% of data&amp;#39;) + theme_minimal()
p4 &amp;lt;- dat %&amp;gt;% sample_n(N/100) %&amp;gt;% 
  ggplot(aes(x=x, y=y)) + geom_point(alpha=0.01) +
  labs(title=&amp;#39;alpha=0.01 with 1% of data&amp;#39;) + theme_minimal()
library(cowplot)
plot_grid(p1,p2,p3,p4,ncol=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kwhkim.github.io/maxR/2022/03/06/better-visualization-of-y-x-for-big-data/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But sampling utilizes only some part of the data. It depends on the chance so the results are different every time we plot.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contidional-density-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contidional density plot&lt;/h2&gt;
&lt;p&gt;There are several reason for plotting. One reason is doing EDA(&lt;strong&gt;E&lt;/strong&gt;xploratory &lt;strong&gt;D&lt;/strong&gt;ata &lt;strong&gt;A&lt;/strong&gt;nalysis) before doing regression analysis such as linear model, ML, and DL.&lt;/p&gt;
&lt;p&gt;The important thing in this case is to see what conditional density &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{p}(y|x)\)&lt;/span&gt; is like. Besides all plots above are focused on bivariate density.&lt;/p&gt;
&lt;p&gt;To visualize the expectation of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; conditional on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; , non-parametric regression line in the following would help.&lt;/p&gt;
&lt;div id=&#34;regression-line&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression line&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- dat %&amp;gt;% sample_n(N/100) %&amp;gt;% 
  ggplot(aes(x=x, y=y)) + geom_point(alpha=0.01) +
  geom_smooth(method=&amp;#39;loess&amp;#39;) + 
  labs(title=&amp;#39;alpha=0.01 with 1% of data, loess&amp;#39;) + theme_minimal() 
p2 &amp;lt;- dat %&amp;gt;% sample_n(N/100) %&amp;gt;% 
  ggplot(aes(x=x, y=y)) + geom_point(alpha=0.01) +
  geom_smooth(method=&amp;#39;auto&amp;#39;) + 
  labs(title=&amp;#39;alpha=0.01 with 1% of data, gam&amp;#39;) + theme_minimal() 
print(p1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(p2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kwhkim.github.io/maxR/2022/03/06/better-visualization-of-y-x-for-big-data/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;70%&#34; /&gt;&lt;img src=&#34;https://kwhkim.github.io/maxR/2022/03/06/better-visualization-of-y-x-for-big-data/index_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can definitely see conditional expectation( &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[y|x] = \int y\  \mathbb{p}(y|x) dy\)&lt;/span&gt; ), but we cannot figure out what the conditional density would be like.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-density&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conditional density&lt;/h3&gt;
&lt;div id=&#34;binning-x&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;binning &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;As we saw above, using small constant &lt;code&gt;alpha&lt;/code&gt; prevents us from identifying the density difference when the points are too gathered and identifying data points where data points are so scarce. One possible solution would be binning &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and sampling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% 
  mutate(xCut = cut(x, breaks=10)) %&amp;gt;%
  group_by(xCut) %&amp;gt;%
  do(sample_n(., 10000, replace=TRUE)) %&amp;gt;%
  ggplot(aes(x=x, y=y)) + 
  geom_point(alpha=0.01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kwhkim.github.io/maxR/2022/03/06/better-visualization-of-y-x-for-big-data/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Better visualizing of conditional density but we can see artifacts. It must be because of too big bin size. Let’s try smaller bin size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% 
  mutate(xCut = cut(x, breaks=50)) %&amp;gt;%
  group_by(xCut) %&amp;gt;%
  do(sample_n(., 2000, replace=TRUE)) %&amp;gt;%
  ggplot(aes(x=x, y=y)) + 
  geom_point(alpha=0.01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kwhkim.github.io/maxR/2022/03/06/better-visualization-of-y-x-for-big-data/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-density-of-x&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Estimating density of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;We can never say what is the best bin size. We would better estimate the probability density function of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We also treated every &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; as identical. We might take estimated probability function of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; into consideration, either using probability function itself or some function(ex. &lt;span class=&#34;math inline&#34;&gt;\(\log\)&lt;/span&gt; ) of it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xDensity &amp;lt;- ks::kde(dat$x)
dat$prob &amp;lt;- predict(xDensity, x = dat$x)

#head(dat)
dat %&amp;gt;% 
  mutate(xCut = cut(x, breaks=50)) %&amp;gt;%
  group_by(xCut) %&amp;gt;%
  do(sample_n(., 2000, replace=TRUE, weight=1/prob)) %&amp;gt;%
  
  ggplot(aes(x=x, y=y)) + 
  geom_point(alpha=0.01) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kwhkim.github.io/maxR/2022/03/06/better-visualization-of-y-x-for-big-data/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <tag>big data</tag>
      
            <tag>visualization</tag>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>character in UTF-8</title>
      <link>https://kwhkim.github.io/maxR/2022/03/06/character-in-utf-8/</link>
      <pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://kwhkim.github.io/maxR/2022/03/06/character-in-utf-8/</guid>
      <description>
&lt;script src=&#34;https://kwhkim.github.io/maxR/2022/03/06/character-in-utf-8/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;encoding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Encoding&lt;/h2&gt;
&lt;p&gt;Computer can store data only with 0s and 1s. Putting together a lot of 0s and 1s, a computer can present a bigger number. But if it want to store a letter, it needs a mapping of a number onto a letter. This mapping is called “&lt;strong&gt;encoding&lt;/strong&gt;”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Encoding&lt;/strong&gt; depends on the letters to store. Letters people use are different for different countries and languages. There are over 1000 encodings worldwide. But over 90% of the encodings used in the internet is UTF-8&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unicode&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Unicode&lt;/h2&gt;
&lt;p&gt;We are in an internet era. It has become ordinary to send documents over the border of a country. But encodings usually were made for use in one country. So the documents from foreign country could be not read properly because the encoding was different&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Unicode was developed for this kind of problem. Unicode tries to have a mapping for all the characters that exist today or existed from the beginning of the history. Unicode Consortium&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; is a non-profit oganization that develops Unicode. The number that a character maps to is called &lt;strong&gt;code point&lt;/strong&gt;. You can check the Code points from &lt;strong&gt;Unicode Code Chart&lt;/strong&gt;&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. As the Unicode version increases, the number of character that Unicode can represent is also increasing&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Version 1.0.0(1991) supported 7129 characters from 24 languages. Version 14(2021) includes 144,697 characters from 159 languages. Version 6.0(2010 decided to support emojis because celluar phone makers demanded and it could have evloved into different encodings for different phone makers because of emojis, which was the reason for Unicode. Unicode tried to incorporate all the characters worldwide so any encodings can be converted to Unicode. Unicode can be materialized into specific encoding scheme such as UTF-8, UTF-16, UTF-32. Those encoding scheme add additional layer for compression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;character-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;character&lt;/code&gt; in R&lt;/h2&gt;
&lt;p&gt;R supports UTF-8. If a &lt;code&gt;character&lt;/code&gt; is not in UTF-8, it can be converted to UTF-8 using the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = &amp;#39;한글&amp;#39; # Hangul in Korean
y = iconv(x, to=&amp;#39;UTF-8&amp;#39;)
x
## [1] &amp;quot;한글&amp;quot;
y
## [1] &amp;quot;한글&amp;quot;
Encoding(x)
## [1] &amp;quot;unknown&amp;quot;
Encoding(y)
## [1] &amp;quot;UTF-8&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;UTF-8 is powerful. It can represent almost any characters. But the font is limited. Most fonts can not display all the characters. So there are cases that characters stored in a vector can not be properly displayed because the font in use does not support.&lt;/p&gt;
&lt;p&gt;I propose the following function &lt;code&gt;u_chars&lt;/code&gt; for inspection of UTF-8 characters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;u_chars&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;u_chars&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The following function &lt;code&gt;u_chars()&lt;/code&gt; utilize the package &lt;code&gt;Unicode&lt;/code&gt; and prints out the information of each character for a string. Unicode characters have labels so characters that can be not displayed properly can be identified.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;u_chars = function(s, encodings) {
  stopifnot(class(s) == &amp;quot;character&amp;quot;)
  stopifnot(length(s)==1)
  if (Encoding(s) == &amp;quot;unknown&amp;quot;) {
    s = iconv(s, to = &amp;#39;UTF-8&amp;#39;)
  } else if (Encoding(s) != &amp;#39;UTF-8&amp;#39;) {
    s = iconv(s, from = Encoding(s), to=&amp;#39;UTF-8&amp;#39;)  } 
  
  dat = data.frame(ch = unlist(strsplit(s, &amp;quot;&amp;quot;))) # split characters
  cps = sapply(dat$ch, utf8ToInt)                # unicode codepoint
  cps_hex = sprintf(&amp;quot;%02x&amp;quot;, cps)                 # convert to hexidecimal number
  
  # hexidecimal numbers are displayed in one of the following styles &amp;quot;  ..ff&amp;quot;, &amp;quot;  a1ff&amp;quot;, &amp;quot;011f3e&amp;quot;
  # first two digits are rarely used so they are shown blank when they are 00
  # The following two digits are 00 when the code is in ASCII so they are shown ..
  cps_hex = 
    ifelse(nchar(cps_hex) &amp;gt; 2,
           stringi::stri_pad(cps_hex, width = 4, side = &amp;#39;left&amp;#39;, pad = &amp;#39;0&amp;#39;),
           stringi::stri_pad(cps_hex, width = 4, side = &amp;#39;left&amp;#39;, pad = &amp;#39;.&amp;#39;))
  dat$codepoint = 
    ifelse(nchar(cps_hex) &amp;gt; 4,
           stringi::stri_pad(cps_hex, width=6, side=&amp;#39;left&amp;#39;, pad=&amp;#39;0&amp;#39;),
           stringi::stri_pad(cps_hex, width=6, side=&amp;#39;left&amp;#39;, pad=&amp;#39; &amp;#39;))
  
  # if given encodings=
  if (!missing(encodings)) {
    for (encoding in encodings) {
      ch_enc = vector(mode=&amp;#39;character&amp;#39;, length=nrow(dat))
      for (i in 1:nrow(dat)) {
        ch = dat$ch[i]
        ch_enc[i] = 
          paste0(sprintf(&amp;quot;%02x&amp;quot;, 
                         as.integer(unlist(
                           iconv(ch, from = &amp;#39;UTF-8&amp;#39;, 
                                     to=encoding, toRaw=TRUE)))),
                 collapse = &amp;#39; &amp;#39;)
      }
      dat$enc = ch_enc
      names(dat)[length(names(dat))] = paste0(&amp;#39;enc.&amp;#39;, encoding)  
    }
  }
  dat$label = Unicode::u_char_label(cps); 
  dat
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;u_chars(&amp;quot;\ufeff\u0041\ub098\u2211\U00010384&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             ch codepoint                     label
## 1     &amp;lt;U+FEFF&amp;gt;      feff ZERO WIDTH NO-BREAK SPACE
## 2            A      ..41    LATIN CAPITAL LETTER A
## 3           나      b098        HANGUL SYLLABLE NA
## 4           ∑      2211           N-ARY SUMMATION
## 5 &amp;lt;U+00010384&amp;gt;    010384     UGARITIC LETTER DELTA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see how the characters will be encoded in other encoding scheme using &lt;code&gt;encodings =&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;u_chars(&amp;quot;\ufeff\u0041똠\u2211\U00010384&amp;quot;, encodings = c(&amp;quot;CP949&amp;quot;, &amp;quot;latin1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             ch codepoint enc.CP949 enc.latin1                     label
## 1     &amp;lt;U+FEFF&amp;gt;      feff                      ZERO WIDTH NO-BREAK SPACE
## 2            A      ..41        41         41    LATIN CAPITAL LETTER A
## 3           똠      b620     8c 63                 HANGUL SYLLABLE DDOM
## 4           ∑      2211     a2 b2                      N-ARY SUMMATION
## 5 &amp;lt;U+00010384&amp;gt;    010384                          UGARITIC LETTER DELTA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;another-application&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Another application&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stringi)
x = &amp;#39;\u0423\u043a\u0440\u0430\u0457\u043d\u0430&amp;#39;
Encoding(x)
## [1] &amp;quot;UTF-8&amp;quot;
#x = iconv(x, to=&amp;#39;UTF-8&amp;#39;) 
cat(x); cat(&amp;#39;\n&amp;#39;)
## Укра&amp;lt;U+0457&amp;gt;на
y = stri_trans_nfd(x)
cat(y); cat(&amp;#39;\n&amp;#39;)
## Укра&amp;lt;U+0456&amp;gt;&amp;lt;U+0308&amp;gt;на
u_chars(x)
##         ch codepoint                     label
## 1       У      0423 CYRILLIC CAPITAL LETTER U
## 2       к      043a  CYRILLIC SMALL LETTER KA
## 3       р      0440  CYRILLIC SMALL LETTER ER
## 4       а      0430   CYRILLIC SMALL LETTER A
## 5 &amp;lt;U+0457&amp;gt;      0457  CYRILLIC SMALL LETTER YI
## 6       н      043d  CYRILLIC SMALL LETTER EN
## 7       а      0430   CYRILLIC SMALL LETTER A
u_chars(y)
##         ch codepoint                                          label
## 1       У      0423                      CYRILLIC CAPITAL LETTER U
## 2       к      043a                       CYRILLIC SMALL LETTER KA
## 3       р      0440                       CYRILLIC SMALL LETTER ER
## 4       а      0430                        CYRILLIC SMALL LETTER A
## 5 &amp;lt;U+0456&amp;gt;      0456 CYRILLIC SMALL LETTER BYELORUSSIAN-UKRAINIAN I
## 6 &amp;lt;U+0308&amp;gt;      0308                            COMBINING DIAERESIS
## 7       н      043d                       CYRILLIC SMALL LETTER EN
## 8       а      0430                        CYRILLIC SMALL LETTER A&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://w3techs.com/technologies/cross/character_encoding/ranking&#34; class=&#34;uri&#34;&gt;https://w3techs.com/technologies/cross/character_encoding/ranking&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Mojibake&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Mojibake&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Unicode_Consortium&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Unicode_Consortium&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.unicode.org/charts/&#34; class=&#34;uri&#34;&gt;http://www.unicode.org/charts/&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;There are several reasons for this. Unicode can embrace new languages or new characters can be found for only embraced languages.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <tag>character</tag>
      
            <tag>preprocessing</tag>
      
            <tag>encoding</tag>
      
      
            <category>R</category>
      
    </item>
    
  </channel>
</rss>